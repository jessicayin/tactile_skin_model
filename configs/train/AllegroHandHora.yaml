
seed: ${..seed}
algo: PPO
network:
  mlp:
    units: [512, 256, 128]
  priv_mlp:
    units: [256, 128, 8]
  point_mlp:
    units: [32, 32, 32]
  contact_mlp:
    units: [32, 32, 32]
  use_point_transformer: False
  use_gt_point_cloud_info: False

load_path: ${..checkpoint} # path to the checkpoint to load
experiment_name: 'experiment_1'
ppo:
  output_name: 'debug'
  multi_gpu: False
  normalize_input: True
  normalize_value: True
  normalize_priv: True
  normalize_point_cloud: True
  value_bootstrap: True
  num_actors: ${...task.env.numEnvs}
  normalize_advantage: True
  gamma: 0.99
  tau: 0.95
  learning_rate: 5e-3
  kl_threshold: 0.02
  # PPO batch collection
  horizon_length: 8
  # minibatch_size: 16384
  # minibatch_size: 512
  minibatch_size: 8
  mini_epochs: 5
  # PPO loss setting
  clip_value: True
  critic_coef: 4
  entropy_coef: 0.0
  e_clip: 0.2
  bounds_loss_coef: 0.0
  distill_loss_coef: 0.0
  # grad clipping
  truncate_grads: True
  grad_norm: 1.0
  # snapshot setting
  # in terms of agent steps (should consider multi-gpu)
  save_best_after: 0
  save_frequency: 10
  max_agent_steps: 3000000000
  # hora setting
  priv_info: False
  priv_info_embed_dim: 8
  proprio_adapt: False
  # asymmetric critic
  asymm_actor_critic: False
  critic_info_dim: 100

padapt:
  action_imitation: False
  cache_suffix: 'stage2'
  with_noisy_obj_quat: False
  noisy_obj_quat_xyz_only: False
  with_debias_obj_xyz: True # only used if with_noisy_obj_quat and noisy_obj_quat_xyz_only
  with_goal_quat: False  # only use in reorientation
  iter_per_step: 1
  expert_fine_contact: False
  visual_distillation: False
  contact_distillation: False
  conv_with_batch_norm: False
  use_deformable_conv: False
  use_temporal_transformer: False
  use_position_encoder: False
  separate_temporal_fusion: False
  use_perceived_point_cloud: False
